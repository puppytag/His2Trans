#!/usr/bin/env python3
"""
compile_commands.json è§£æå™¨

ä» compile_commands.json æå–æºæ–‡ä»¶çš„å¤´æ–‡ä»¶æœç´¢è·¯å¾„ï¼ˆ-I å‚æ•°ï¼‰
ç”¨äºä¼˜åŒ– bindgen å’Œé¢„å¤„ç†è¿‡ç¨‹

å¢å¼ºåŠŸèƒ½ (2025-12-05):
- å…¨å±€å¤´æ–‡ä»¶ç´¢å¼• (Global Header Indexing)
- æ™ºèƒ½å¯»è·¯ï¼šå½“ bindgen æŠ¥é”™æ‰¾ä¸åˆ°å¤´æ–‡ä»¶æ—¶ï¼Œè‡ªåŠ¨ä»ç´¢å¼•ä¸­æŸ¥æ‰¾
- åŸºäº Rustine è®ºæ–‡çš„ Build-Database Guided Discovery æ–¹æ³•
"""

import json
import re
import hashlib
import pickle
import shlex
import subprocess
import tempfile
import shutil
from pathlib import Path
from typing import List, Optional, Dict, Set, Tuple, Any, Iterable
from collections import defaultdict, deque
from dataclasses import dataclass, field
from enum import Enum
import logging
import os

logger = logging.getLogger(__name__)


# ========== é¢„å¤„ç†ä¸Šä¸‹æ–‡é€‰æ‹©ç­–ç•¥ ==========

class ContextSelectionStrategy(Enum):
    """ä¸Šä¸‹æ–‡é€‰æ‹©ç­–ç•¥"""
    ACTIVE = "active"      # ç”¨æˆ·æŒ‡å®šç›®æ ‡ board/mode
    BEST = "best"          # é€‰æ‹©äº§ç”Ÿå‡½æ•°æœ€å¤š/å®æœ€å®Œæ•´çš„ç¼–è¯‘å‘½ä»¤
    UNION = "union"        # å¤šæ¡å‘½ä»¤å–å¹¶é›†ï¼ˆéœ€è¦ #[cfg] ç®¡ç†ï¼‰


@dataclass
class PreprocessingContext:
    """é¢„å¤„ç†ä¸Šä¸‹æ–‡ï¼šä¸€ä¸ªæºæ–‡ä»¶çš„ç¼–è¯‘é…ç½®"""
    source_file: Path
    entry: Dict                          # compile_commands.json ä¸­çš„æ¡ç›®
    preprocessed_file: Optional[Path] = None  # é¢„å¤„ç†åçš„ .i æ–‡ä»¶è·¯å¾„
    function_count: int = 0              # é¢„å¤„ç†åçš„å‡½æ•°æ•°é‡ï¼ˆç”¨äº best ç­–ç•¥ï¼‰
    macro_count: int = 0                 # å®å®šä¹‰æ•°é‡ï¼ˆç”¨äº best ç­–ç•¥ï¼‰
    line_mapping: Dict[int, Tuple[str, int]] = field(default_factory=dict)  # .i æ–‡ä»¶è¡Œå· -> (åŸæ–‡ä»¶, åŸè¡Œå·)
    error: Optional[str] = None          # é¢„å¤„ç†é”™è¯¯ä¿¡æ¯
    # Proxy-TU: when `source_file` has no compile_commands entry, we may reuse a nearby TU's flags.
    proxy_used: bool = False
    proxy_entry_file: Optional[str] = None
    proxy_reason: Optional[str] = None
    # Best-effort diagnostics when clang -E fails.
    auto_resolve: Optional[Dict[str, Any]] = None


class CompileCommandsParser:
    """ç¼–è¯‘æ•°æ®åº“è§£æå™¨"""
    
    # ç±»çº§åˆ«çš„ç¼“å­˜ç›®å½•
    _cache_dir = None
    
    @classmethod
    def _get_cache_dir(cls) -> Path:
        """è·å–ç¼“å­˜ç›®å½•
        
        é»˜è®¤ç¼“å­˜ä½ç½®ï¼šé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .cache/compile_commands/
        æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ C2R_CACHE_ROOT å°†ç¼“å­˜æ”¾åˆ°æŸæ¬¡è¿è¡Œç›®å½•ä¸‹ï¼Œä¾¿äºå®éªŒéš”ç¦»ä¸æ¸…ç†ï¼š
          C2R_CACHE_ROOT=<run>/intermediate/cache
        """
        if cls._cache_dir is None:
            cache_root_env = os.environ.get("C2R_CACHE_ROOT", "").strip()
            if cache_root_env:
                cls._cache_dir = Path(cache_root_env).expanduser().resolve() / "compile_commands"
            else:
                # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆè„šæœ¬æ‰€åœ¨ç›®å½•ï¼‰
                project_root = Path(__file__).parent.resolve()
                cls._cache_dir = project_root / ".cache" / "compile_commands"
            cls._cache_dir.mkdir(parents=True, exist_ok=True)
        return cls._cache_dir
    
    def __init__(self, compile_db_path: Path, ohos_root: Path = None):
        """
        åˆå§‹åŒ–è§£æå™¨
        
        Args:
            compile_db_path: compile_commands.json çš„è·¯å¾„
            ohos_root: OpenHarmony æºç æ ¹ç›®å½•ï¼ˆç”¨äºè·¯å¾„è§„èŒƒåŒ–ï¼‰
        """
        self.compile_db_path = Path(compile_db_path)
        self.ohos_root = Path(ohos_root) if ohos_root else None
        self.compile_db = None
        self.file_index = {}  # æ–‡ä»¶å -> æ¡ç›®åˆ—è¡¨çš„ç´¢å¼•
        self._all_include_dirs_cache = None  # ç¼“å­˜æ‰€æœ‰ include è·¯å¾„
        
        # ========== å¢å¼º: å…¨å±€å¤´æ–‡ä»¶ç´¢å¼• (Global Header Indexing) ==========
        # ç”¨äºå¿«é€ŸæŸ¥æ‰¾å¤´æ–‡ä»¶çš„çœŸå®è·¯å¾„
        # æ ¼å¼: { "filename.h": ["/path/to/dir1", "/path/to/dir2"] }
        self._header_index = None  # æ‡’åŠ è½½ï¼Œé¿å…å¯åŠ¨å¤ªæ…¢
        self._header_index_built = False
        # Missing-header resolution cache (per compile_db): header -> {success: str|None, candidates: [str,...]}
        self._header_resolution_cache: Dict[str, Dict[str, Any]] = {}
        self._header_resolution_cache_loaded: bool = False
        
        self._load_database()

    def _resolve_entry_directory(self, entry: Dict) -> Path:
        """
        Resolve entry['directory'] to an absolute path.

        Why:
        - For portability, some open-source layouts keep compile_commands entries with
          relative "directory" (e.g. ".").
        - Such paths should be interpreted relative to the compile_commands.json location,
          not the current process working directory.
        """
        base_dir = Path(entry.get("directory", ".") or ".")
        if base_dir.is_absolute():
            try:
                return base_dir.resolve()
            except Exception:
                return base_dir
        try:
            return (self.compile_db_path.parent / base_dir).resolve()
        except Exception:
            return (self.compile_db_path.parent / base_dir)

    def _get_header_resolution_cache_file(self) -> Path:
        """Per-compile_db cache for missing header resolution results (small JSON; avoids rewriting huge pickle cache)."""
        cache_key = hashlib.md5(str(self.compile_db_path.resolve()).encode()).hexdigest()
        return self._get_cache_dir() / f"{cache_key}.header_resolutions.json"

    def _load_header_resolution_cache(self) -> None:
        if self._header_resolution_cache_loaded:
            return
        self._header_resolution_cache_loaded = True
        p = self._get_header_resolution_cache_file()
        if not p.exists():
            self._header_resolution_cache = {}
            return
        try:
            data = json.loads(p.read_text(encoding="utf-8", errors="ignore") or "")
        except Exception:
            self._header_resolution_cache = {}
            return
        if not isinstance(data, dict):
            self._header_resolution_cache = {}
            return
        if str(data.get("compile_db_path") or "") != str(self.compile_db_path.resolve()):
            self._header_resolution_cache = {}
            return
        resolved = data.get("resolved")
        if not isinstance(resolved, dict):
            self._header_resolution_cache = {}
            return
        out: Dict[str, Dict[str, Any]] = {}
        for k, v in resolved.items():
            if not isinstance(k, str) or not k.strip():
                continue
            if not isinstance(v, dict):
                continue
            entry: Dict[str, Any] = {}
            success = v.get("success")
            if isinstance(success, str) and success.strip():
                entry["success"] = success.strip()
            candidates = v.get("candidates")
            if isinstance(candidates, list):
                entry["candidates"] = [str(x) for x in candidates if isinstance(x, str) and x.strip()]
            if entry:
                out[k] = entry
        self._header_resolution_cache = out

    def _save_header_resolution_cache(self) -> None:
        try:
            p = self._get_header_resolution_cache_file()
            payload = {
                "version": 1,
                "compile_db_path": str(self.compile_db_path.resolve()),
                "resolved": self._header_resolution_cache,
            }
            p.parent.mkdir(parents=True, exist_ok=True)
            p.write_text(json.dumps(payload, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")
        except Exception as e:
            logger.debug(f"ä¿å­˜ header resolution cache å¤±è´¥: {e}")

    def _get_openharmony_search_root(self) -> Optional[Path]:
        """
        Return the preferred search root for missing headers.

        Requirement: search should be rooted at the `OpenHarmony/` directory when available.
        """
        if not self.ohos_root:
            return None
        try:
            root = self.ohos_root.resolve()
        except Exception:
            root = self.ohos_root
        if root.name != "OpenHarmony":
            try:
                cand = (root / "OpenHarmony").resolve()
                if cand.exists():
                    return cand
            except Exception:
                pass
        return root
    
    def _get_cache_file(self) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜æ–‡ä»¶å
        cache_key = hashlib.md5(str(self.compile_db_path.resolve()).encode()).hexdigest()
        return self._get_cache_dir() / f"{cache_key}.cache"
    
    def _is_cache_valid(self, cache_file: Path) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not cache_file.exists():
            return False
        
        if not self.compile_db_path.exists():
            return False
        
        try:
            # æ£€æŸ¥æºæ–‡ä»¶ä¿®æ”¹æ—¶é—´
            source_mtime = self.compile_db_path.stat().st_mtime
            cache_mtime = cache_file.stat().st_mtime
            
            # å¦‚æœæºæ–‡ä»¶æ¯”ç¼“å­˜æ–°ï¼Œç¼“å­˜æ— æ•ˆ
            if source_mtime > cache_mtime:
                return False
            
            return True
        except Exception:
            return False
    
    def _load_cache(self, cache_file: Path) -> Optional[Dict]:
        """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            print(f"    [ç¼“å­˜] æ­£åœ¨åŠ è½½ç¼“å­˜: {cache_file.name}")
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # éªŒè¯ç¼“å­˜æ•°æ®æ ¼å¼
            if not isinstance(cache_data, dict):
                return None
            
            required_keys = ['compile_db', 'file_index', 'compile_db_path']
            if not all(key in cache_data for key in required_keys):
                return None
            
            # éªŒè¯ç¼“å­˜æ˜¯å¦å¯¹åº”åŒä¸€ä¸ªæ–‡ä»¶
            if cache_data['compile_db_path'] != str(self.compile_db_path.resolve()):
                return None
            
            print(f"    âœ“ ç¼“å­˜åŠ è½½æˆåŠŸ")
            return cache_data
        except Exception as e:
            logger.debug(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def _save_cache(self, cache_file: Path, data: Dict):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶"""
        try:
            cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            print(f"    âœ“ ç¼“å­˜å·²ä¿å­˜: {cache_file.name}")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _load_database(self):
        """åŠ è½½ç¼–è¯‘æ•°æ®åº“ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        print(f"  [åŠ è½½] æ­£åœ¨åŠ è½½ compile_commands.json: {self.compile_db_path}")
        if not self.compile_db_path.exists():
            logger.warning(f"compile_commands.json ä¸å­˜åœ¨: {self.compile_db_path}")
            print(f"  âœ— æ–‡ä»¶ä¸å­˜åœ¨")
            self.compile_db = []
            return
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cache_file = self._get_cache_file()
        if self._is_cache_valid(cache_file):
            cache_data = self._load_cache(cache_file)
            if cache_data:
                self.compile_db = cache_data['compile_db']
                self.file_index = cache_data['file_index']
                print(f"  âœ“ ä»ç¼“å­˜åŠ è½½: {len(self.compile_db)} ä¸ªç¼–è¯‘æ¡ç›®")
                logger.info(f"ä»ç¼“å­˜åŠ è½½ compile_commands.json: {len(self.compile_db)} ä¸ªæ¡ç›®")
                return
        
        # ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°åŠ è½½
        print(f"    [ç¼“å­˜] ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è§£æ...")
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = self.compile_db_path.stat().st_size
            print(f"    æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.2f} MB")
            
            print(f"    æ­£åœ¨è¯»å– JSON æ•°æ®...")
            with open(self.compile_db_path, 'r', encoding='utf-8') as f:
                self.compile_db = json.load(f)
            
            print(f"  âœ“ æˆåŠŸåŠ è½½: {len(self.compile_db)} ä¸ªç¼–è¯‘æ¡ç›®")
            logger.info(f"åŠ è½½ compile_commands.json: {len(self.compile_db)} ä¸ªæ¡ç›®")
            
            # å»ºç«‹æ–‡ä»¶åç´¢å¼•ï¼ˆåŠ é€ŸæŸ¥æ‰¾ï¼‰
            print(f"    æ­£åœ¨å»ºç«‹æ–‡ä»¶åç´¢å¼•...")
            self._build_file_index()
            print(f"  âœ“ ç´¢å¼•å»ºç«‹å®Œæˆ: {len(self.file_index)} ä¸ªå”¯ä¸€æ–‡ä»¶å")
            
            # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆåŸºç¡€æ•°æ®ï¼Œinclude è·¯å¾„ä¼šåœ¨ get_all_include_dirs æ—¶æ·»åŠ ï¼‰
            cache_data = {
                'compile_db': self.compile_db,
                'file_index': self.file_index,
                'compile_db_path': str(self.compile_db_path.resolve())
            }
            self._save_cache(cache_file, cache_data)
            
        except Exception as e:
            logger.error(f"åŠ è½½ compile_commands.json å¤±è´¥: {e}")
            print(f"  âœ— åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.compile_db = []
    
    def _build_file_index(self):
        """å»ºç«‹æ–‡ä»¶ååˆ°æ¡ç›®çš„ç´¢å¼•"""
        self.file_index = {}
        for entry in self.compile_db:
            file_path = entry.get('file', '')
            if not file_path:
                continue
            
            file_name = Path(file_path).name
            if file_name not in self.file_index:
                self.file_index[file_name] = []
            self.file_index[file_name].append(entry)
    
    def get_includes_for_file(
        self, 
        source_file: Path,
        normalize_paths: bool = True
    ) -> List[Path]:
        """
        è·å–æºæ–‡ä»¶çš„å¤´æ–‡ä»¶æœç´¢è·¯å¾„ï¼ˆ-I å‚æ•°ï¼‰
        
        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ï¼‰
            normalize_paths: æ˜¯å¦è§„èŒƒåŒ–è·¯å¾„ï¼ˆç¡®ä¿è·¯å¾„å­˜åœ¨ï¼‰
        
        Returns:
            å¤´æ–‡ä»¶æœç´¢è·¯å¾„åˆ—è¡¨ï¼ˆPath å¯¹è±¡ï¼‰
        """
        if not self.compile_db:
            return []
        
        source_file = Path(source_file).resolve()
        source_file_str = str(source_file)
        source_file_name = source_file.name
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ¡ç›®
        matched_entry = self._find_matching_entry(source_file, source_file_str, source_file_name)
        
        if not matched_entry:
            logger.debug(f"æœªæ‰¾åˆ°ç¼–è¯‘å‘½ä»¤: {source_file_name}")
            return []
        
        # æå– -I å‚æ•°
        command = matched_entry.get('command', '')
        if not command:
            return []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰ -I è·¯å¾„
        # åŒ¹é…æ¨¡å¼: -I/path æˆ– -I /path
        include_pattern = r'-I\s*([^\s]+)'
        include_paths = re.findall(include_pattern, command)
        
        # è½¬æ¢ä¸º Path å¯¹è±¡å¹¶è§„èŒƒåŒ–
        result = []
        base_dir = self._resolve_entry_directory(matched_entry)
        
        for inc_path_str in include_paths:
            inc_path = self._normalize_path(inc_path_str, base_dir)
            
            # å¦‚æœ normalize_paths=Trueï¼Œåªè¿”å›å­˜åœ¨çš„è·¯å¾„
            if normalize_paths:
                if inc_path.exists():
                    result.append(inc_path)
            else:
                result.append(inc_path)
        
        return result

    def get_clang_flags_for_file(
        self,
        source_file: Path,
        normalize_paths: bool = True,
    ) -> List[str]:
        """
        è·å–æŸä¸ªæºæ–‡ä»¶å¯¹åº”çš„ clang å‚æ•°ï¼ˆç”¨äº bindgen/clang -Eï¼‰ï¼Œå¹¶å°½é‡ä¿æŒå‚æ•°é¡ºåºã€‚

        ä¸ get_includes_for_file çš„åŒºåˆ«ï¼š
        - ä¸ä»…åŒ…å« -Iï¼Œè¿˜ä¼šåŒ…å« -isystem/-D/-U/--sysroot/-include ç­‰ä¸è§£æå¤´æ–‡ä»¶/å®ç›¸å…³çš„å‚æ•°
        - ä¿ç•™ compile_commands ä¸­çš„åŸå§‹é¡ºåºï¼ˆå¯¹å¤´æ–‡ä»¶é€‰æ‹©å¾ˆå…³é”®ï¼‰

        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ï¼‰
            normalize_paths: æ˜¯å¦è§„èŒƒåŒ–è·¯å¾„ï¼ˆç¡®ä¿è·¯å¾„å­˜åœ¨ã€ç›¸å¯¹è·¯å¾„æŒ‰ entry.directory/ohos_root è§£æï¼‰

        Returns:
            clang å‚æ•°åˆ—è¡¨ï¼ˆå¯ç›´æ¥æ‹¼æ¥åˆ° bindgen çš„ `--` ä¹‹åï¼‰
        """
        if not self.compile_db:
            return []

        source_file = Path(source_file).resolve()
        source_file_str = str(source_file)
        source_file_name = source_file.name

        matched_entry = self._find_matching_entry(source_file, source_file_str, source_file_name)
        if not matched_entry:
            logger.debug(f"æœªæ‰¾åˆ°ç¼–è¯‘å‘½ä»¤: {source_file_name}")
            return []

        return self.get_clang_flags_for_entry(matched_entry, normalize_paths=normalize_paths)

    def get_clang_flags_for_entry(
        self,
        entry: Dict,
        normalize_paths: bool = True,
    ) -> List[str]:
        """
        ä» compile_commands.json çš„æŸä¸ª entry ä¸­æå– clang å‚æ•°ï¼ˆç”¨äº bindgen/clang -Eï¼‰ï¼Œå¹¶å°½é‡ä¿æŒå‚æ•°é¡ºåºã€‚

        è¯´æ˜ï¼š
        - è¿™æ˜¯ preprocess-first çš„å…³é”®ï¼šåŒä¸€æºæ–‡ä»¶å¯èƒ½åœ¨ compile_commands é‡Œæœ‰å¤šæ¡ entryï¼ˆä¸åŒå®/å‚æ•°ï¼‰ï¼Œ
          ä¸Šä¸‹æ–‡é€‰æ‹©åå¿…é¡»ä¸¥æ ¼ä½¿ç”¨â€œè¢«é€‰ä¸­çš„ entryâ€çš„å‚æ•°ï¼Œè€Œä¸èƒ½å†æ ¹æ®æ–‡ä»¶å/è·¯å¾„é‡æ–°åŒ¹é… entryã€‚
        """
        if not entry:
            return []

        args: List[str] = []
        if isinstance(entry.get("arguments"), list) and entry.get("arguments"):
            args = list(entry.get("arguments"))
        else:
            command = entry.get("command", "") or ""
            if not command:
                return []
            try:
                args = shlex.split(command, posix=True)
            except Exception:
                # æç«¯æƒ…å†µä¸‹ä¿åº•ï¼›ä¸ä¼šå¾ˆå‡†ç¡®ï¼Œä½†æ¯”ç›´æ¥å¤±è´¥å¥½
                args = command.split()

        base_dir = self._resolve_entry_directory(entry)

        def _fallback_sysroot_path(sysroot_path: Path) -> Optional[Path]:
            """
            Some OHOS compile_commands entries point to a product-scoped sysroot like:
              out/<out_dir>/<product>/sysroot
            which may not exist unless you run further build steps (ninja/actions).

            For preprocessing/bindgen we can often fall back to a board-scoped sysroot that *does* exist,
            e.g.:
              out/<out_dir>/obj/third_party/musl
              out/<out_dir>/sdk-native/os-irrelevant/sysroot
              out/<out_dir>/NOTICE_FILES/ndk/sysroot
            """
            try:
                parts = sysroot_path.parts
                if "out" not in parts:
                    return None
                idx = parts.index("out")
                if idx + 1 >= len(parts):
                    return None
                out_root = Path(*parts[: idx + 2])  # .../out/<out_dir>

                candidates = [
                    out_root / "obj" / "third_party" / "musl",
                    out_root / "sdk-native" / "os-irrelevant" / "sysroot",
                    out_root / "NOTICE_FILES" / "ndk" / "sysroot",
                ]
                for cand in candidates:
                    if not cand.exists():
                        continue
                    # Prefer a sysroot that actually contains headers.
                    if (cand / "usr" / "include").exists() or (cand / "include").exists():
                        return cand
                # Final fallback: use OpenHarmony SDK sysroot (better than host /usr/include).
                # This helps when a profile was generated with `--build-only-gn` and product sysroot
                # wasn't materialized under out/<out_dir>/... yet.
                if self.ohos_root:
                    sdk_root = (self.ohos_root / "prebuilts" / "ohos-sdk" / "linux").resolve()
                    if sdk_root.exists():
                        versions = []
                        for p in sdk_root.iterdir():
                            if p.is_dir() and p.name.isdigit():
                                versions.append(p)
                        versions.sort(key=lambda p: int(p.name), reverse=True)
                        for v in versions:
                            cand = v / "native" / "sysroot"
                            if cand.exists() and (cand / "usr" / "include").exists():
                                return cand
            except Exception:
                return None
            return None

        def _norm_path(p: str) -> Optional[str]:
            if not p:
                return None
            try:
                normalized = self._normalize_path(p, base_dir)
                if normalize_paths and not normalized.exists():
                    return None
                return str(normalized)
            except Exception:
                return None

        clang_flags: List[str] = []

        i = 0
        while i < len(args):
            a = args[i]

            # -I /path  or  -I/path
            if a == "-I":
                if i + 1 < len(args):
                    p = _norm_path(args[i + 1])
                    if p:
                        clang_flags.extend(["-I", p])
                    i += 2
                    continue
            if a.startswith("-I") and a != "-I":
                p = _norm_path(a[2:])
                if p:
                    clang_flags.extend(["-I", p])
                i += 1
                continue

            # -isystem /path
            if a == "-isystem":
                if i + 1 < len(args):
                    p = _norm_path(args[i + 1])
                    if p:
                        clang_flags.extend(["-isystem", p])
                    i += 2
                    continue
            if a.startswith("-isystem") and a != "-isystem":
                p = _norm_path(a[len("-isystem"):])
                if p:
                    clang_flags.extend(["-isystem", p])
                i += 1
                continue

            # -D NAME or -DNAME
            if a == "-D":
                if i + 1 < len(args):
                    clang_flags.append(f"-D{args[i + 1]}")
                    i += 2
                    continue
            if a.startswith("-D") and a != "-D":
                clang_flags.append(a)
                i += 1
                continue

            # -U NAME or -UNAME
            if a == "-U":
                if i + 1 < len(args):
                    clang_flags.append(f"-U{args[i + 1]}")
                    i += 2
                    continue
            if a.startswith("-U") and a != "-U":
                clang_flags.append(a)
                i += 1
                continue

            # -include file
            if a == "-include":
                if i + 1 < len(args):
                    p = _norm_path(args[i + 1])
                    if p:
                        clang_flags.extend(["-include", p])
                    i += 2
                    continue

            # -imacros file
            if a == "-imacros":
                if i + 1 < len(args):
                    p = _norm_path(args[i + 1])
                    if p:
                        clang_flags.extend(["-imacros", p])
                    i += 2
                    continue

            # --sysroot=...  or  --sysroot ...
            if a.startswith("--sysroot="):
                raw = a.split("=", 1)[1]
                try:
                    normalized = self._normalize_path(raw, base_dir)
                except Exception:
                    normalized = None
                if normalized is not None:
                    if normalize_paths and not normalized.exists():
                        fb = _fallback_sysroot_path(normalized)
                        if fb:
                            clang_flags.append(f"--sysroot={str(fb)}")
                    else:
                        clang_flags.append(f"--sysroot={str(normalized)}")
                i += 1
                continue
            if a == "--sysroot":
                if i + 1 < len(args):
                    raw = args[i + 1]
                    try:
                        normalized = self._normalize_path(raw, base_dir)
                    except Exception:
                        normalized = None
                    if normalized is not None:
                        if normalize_paths and not normalized.exists():
                            fb = _fallback_sysroot_path(normalized)
                            if fb:
                                clang_flags.append(f"--sysroot={str(fb)}")
                        else:
                            clang_flags.append(f"--sysroot={str(normalized)}")
                    i += 2
                    continue

            # -isysroot /path
            if a == "-isysroot":
                if i + 1 < len(args):
                    raw = args[i + 1]
                    try:
                        normalized = self._normalize_path(raw, base_dir)
                    except Exception:
                        normalized = None
                    if normalized is not None:
                        if normalize_paths and not normalized.exists():
                            fb = _fallback_sysroot_path(normalized)
                            if fb:
                                clang_flags.extend(["-isysroot", str(fb)])
                        else:
                            clang_flags.extend(["-isysroot", str(normalized)])
                    i += 2
                    continue

            # -target triple
            if a == "-target":
                if i + 1 < len(args):
                    clang_flags.extend(["-target", args[i + 1]])
                    i += 2
                    continue
            # --target triple  or  --target=triple (clang also supports this form; OHOS compile_commands uses it)
            if a == "--target":
                if i + 1 < len(args):
                    clang_flags.append(f"--target={args[i + 1]}")
                    i += 2
                    continue
            if a.startswith("--target="):
                clang_flags.append(a)
                i += 1
                continue

            # Common arch/CPU flags that affect builtin macros/types (helpful for preprocessing/bindgen)
            # Keep them verbatim (no path normalization needed).
            if a in ("-mthumb", "-marm"):
                clang_flags.append(a)
                i += 1
                continue
            if a.startswith((
                "-march=", "-mcpu=", "-mfpu=", "-mfloat-abi=", "-mtune=", "-mabi=",
                "-mno-", "-mno_", "-msoft-float", "-mhard-float",
            )):
                clang_flags.append(a)
                i += 1
                continue

            # å…¶ä½™å‚æ•°å¿½ç•¥ï¼ˆ-O/-g/-c/-o/-M* ç­‰å¯¹ bindgen è§£ææ„ä¹‰ä¸å¤§ä¸”å¯èƒ½å¼•å…¥å™ªå£°ï¼‰
            i += 1

        return clang_flags

    def _resolve_entry_file_path(self, entry: Dict) -> Optional[Path]:
        """
        Resolve compile_commands entry['file'] to an absolute path (best-effort).

        Notes:
        - compile_commands.json entries often store file paths relative to entry['directory'].
        - We resolve relative paths against entry['directory'] first, then against ohos_root as a fallback.
        """
        entry_file = entry.get("file") or ""
        if not entry_file:
            return None

        entry_dir = self._resolve_entry_directory(entry)
        rel = Path(entry_file)
        path = rel
        if not path.is_absolute():
            path = entry_dir / rel
            # Fallback: some databases store paths relative to OpenHarmony root.
            if not path.exists() and self.ohos_root:
                path = self.ohos_root / rel

        try:
            return path.resolve(strict=False)
        except Exception:
            return path

    def get_entry_for_file_with_reason(self, source_file: Path) -> Tuple[Optional[Dict], Dict[str, Any]]:
        """
        Get the matched compile_commands entry for a source file, plus a small match-reason dict.

        This is primarily for diagnostics (e.g. bindgen failures) so logs can show whether the
        match was exact-path, suffix-path, or a fallback-by-filename candidate.
        """
        if not self.compile_db:
            return None, {"reason": "empty_compile_db", "candidates": 0, "suffix_len": None}

        source_file = Path(source_file).resolve()
        source_file_str = str(source_file)
        source_file_name = source_file.name
        return self._find_matching_entry_with_info(source_file, source_file_str, source_file_name)

    def _find_matching_entry(
        self, 
        source_file: Path, 
        source_file_str: str,
        source_file_name: str
    ) -> Optional[Dict]:
        """æŸ¥æ‰¾åŒ¹é…çš„ç¼–è¯‘æ•°æ®åº“æ¡ç›®"""
        entry, _info = self._find_matching_entry_with_info(source_file, source_file_str, source_file_name)
        return entry

    def _find_matching_entry_with_info(
        self,
        source_file: Path,
        source_file_str: str,
        source_file_name: str,
    ) -> Tuple[Optional[Dict], Dict[str, Any]]:
        """æŸ¥æ‰¾åŒ¹é…çš„ç¼–è¯‘æ•°æ®åº“æ¡ç›®ï¼ˆå¸¦è¯Šæ–­ä¿¡æ¯ï¼‰"""

        info: Dict[str, Any] = {"reason": "none", "candidates": 0, "suffix_len": None}

        def _return(entry: Optional[Dict], reason: str, suffix_len: Optional[int] = None) -> Tuple[Optional[Dict], Dict[str, Any]]:
            info["reason"] = reason
            info["suffix_len"] = suffix_len
            if entry:
                info["matched_entry_file"] = entry.get("file")
                info["matched_entry_directory"] = entry.get("directory")
            return entry, info

        def _entry_score(entry: Dict) -> Tuple[int, int, int]:
            """
            Heuristic scoring for choosing between multiple compile_commands entries for the same file.

            Common case in OpenHarmony:
            - one entry is host toolchain (clang_x64)
            - another is target toolchain (arm-linux-ohos, with --sysroot)
            For bindgen/preprocess we usually want the target one.
            """
            directory = (entry.get("directory") or "").replace("\\", "/")
            args = entry.get("arguments")
            if isinstance(args, list) and args:
                text = " ".join(str(a) for a in args)
            else:
                text = str(entry.get("command") or "")

            text_norm = text.replace("\\", "/")

            has_target = ("--target=" in text_norm) or (" -target " in f" {text_norm} ")
            is_ohos_target = has_target and ("ohos" in text_norm)
            has_sysroot = ("--sysroot=" in text_norm) or (" --sysroot " in f" {text_norm} ") or (" -isysroot " in f" {text_norm} ")

            host_hint = (
                "/clang_x64/" in directory
                or "/clang_x64/" in text_norm
                or "x86_64-linux-gnu" in text_norm
                or "--target=x86_64" in text_norm
            )

            return (
                1 if is_ohos_target else 0,
                1 if has_sysroot else 0,
                0 if host_hint else 1,
            )

        def _pick_best_entry(entries: List[Dict], *, reason: str, suffix_len: Optional[int] = None) -> Tuple[Optional[Dict], Dict[str, Any]]:
            if not entries:
                return _return(None, reason, suffix_len=suffix_len)
            if len(entries) == 1:
                return _return(entries[0], reason, suffix_len=suffix_len)
            scored = [(_entry_score(e), e) for e in entries]
            scored.sort(key=lambda x: x[0], reverse=True)
            best_score, best_entry = scored[0]
            info["multi_entry_match"] = True
            info["multi_entry_count"] = len(entries)
            info["picked_score"] = list(best_score)
            # keep only a small sample for diagnostics
            info["picked_score_samples"] = [list(s) for s, _e in scored[:5]]
            return _return(best_entry, reason, suffix_len=suffix_len)

        # ç­–ç•¥1: æ–‡ä»¶ååŒ¹é…ï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰+ ç»“åˆ entry.directory è¿›è¡Œç»å¯¹è·¯å¾„å¯¹é½
        if source_file_name in self.file_index:
            candidates = self.file_index[source_file_name]
            info["candidates"] = len(candidates)
            
            # ä¼˜å…ˆåŒ¹é…ï¼šè·¯å¾„ç›¸ä¼¼åº¦é«˜çš„
            exact_matches: List[Dict] = []
            for entry in candidates:
                entry_path = self._resolve_entry_file_path(entry)
                if entry_path and entry_path == source_file:
                    exact_matches.append(entry)
            if exact_matches:
                return _pick_best_entry(exact_matches, reason="filename_index_exact_path", suffix_len=None)
            
            # ç­–ç•¥1.2: æœ«å°¾è·¯å¾„ç‰‡æ®µåŒ¹é…ï¼ˆé™ä½åŒåæ–‡ä»¶ç¢°æ’è¯¯é€‰æ¦‚ç‡ï¼‰
            source_parts = source_file.parts
            for suffix_len in (4, 3, 2):
                if len(source_parts) < suffix_len:
                    continue
                suffix = source_parts[-suffix_len:]
                suffix_matches: List[Dict] = []
                for entry in candidates:
                    entry_path = self._resolve_entry_file_path(entry)
                    if not entry_path:
                        continue
                    parts = entry_path.parts
                    if len(parts) >= suffix_len and parts[-suffix_len:] == suffix:
                        suffix_matches.append(entry)
                if suffix_matches:
                    return _pick_best_entry(suffix_matches, reason="filename_index_suffix_path", suffix_len=suffix_len)

            # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå€™é€‰ï¼ˆå¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„/åŒåç¢°æ’ï¼‰
            if candidates:
                # Still prefer target/sysroot style entries (reduces host/target mixups).
                return _pick_best_entry(list(candidates), reason="filename_index_first_candidate", suffix_len=None)
            return _return(None, "filename_index_no_candidates")
        
        # ç­–ç•¥2: æ— ç´¢å¼•å‘½ä¸­æ—¶çš„ä¿åº•æ‰«æï¼ˆé€šå¸¸ä¸ä¼šèµ°åˆ°è¿™é‡Œï¼‰
        for entry in self.compile_db:
            entry_path = self._resolve_entry_file_path(entry)
            if entry_path and str(entry_path) == source_file_str:
                return _return(entry, "full_scan_exact_str")

        return _return(None, "no_match")
    
    def _normalize_path(self, path_str: str, base_dir: Path) -> Path:
        """
        è§„èŒƒåŒ–è·¯å¾„
        
        - å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        - å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äº base_dir
        - å¦‚æœè·¯å¾„ä¸å­˜åœ¨ä¸”æä¾›äº† ohos_rootï¼Œå°è¯•ç›¸å¯¹äº ohos_root
        """
        path = Path(path_str)
        
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        if path.is_absolute():
            return path.resolve()
        
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äº base_dir
        result = (base_dir / path).resolve()
        
        # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ä¸”æä¾›äº† ohos_rootï¼Œå°è¯•ç›¸å¯¹äº ohos_root
        if not result.exists() and self.ohos_root:
            result = (self.ohos_root / path).resolve()
        
        return result
    
    def get_all_include_dirs(self, source_files: List[Path] = None) -> Set[Path]:
        """
        è·å–æ‰€æœ‰æºæ–‡ä»¶çš„ include è·¯å¾„ï¼ˆå»é‡ï¼‰
        
        ä¼˜åŒ–ï¼š
        1. ä½¿ç”¨å†…å­˜ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
        2. ç›´æ¥ä» compile_db ä¸­æå–æ‰€æœ‰ -I è·¯å¾„ï¼Œé¿å…é€ä¸ªæ–‡ä»¶æŸ¥æ‰¾
        
        Args:
            source_files: æºæ–‡ä»¶åˆ—è¡¨ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼‰
        
        Returns:
            æ‰€æœ‰ include è·¯å¾„çš„é›†åˆ
        """
        # å¦‚æœå·²ç»ç¼“å­˜ï¼Œç›´æ¥è¿”å›
        if self._all_include_dirs_cache is not None:
            return self._all_include_dirs_cache
        
        # å°è¯•ä»ç¼“å­˜æ–‡ä»¶åŠ è½½
        cache_file = self._get_cache_file()
        if cache_file.exists():
            try:
                cache_data = self._load_cache(cache_file)
                if cache_data and 'all_include_dirs' in cache_data:
                    self._all_include_dirs_cache = set(Path(p) for p in cache_data['all_include_dirs'])
                    print(f"    âœ“ ä»ç¼“å­˜åŠ è½½ include è·¯å¾„: {len(self._all_include_dirs_cache)} ä¸ª")
                    return self._all_include_dirs_cache
            except Exception as e:
                logger.debug(f"ä»ç¼“å­˜åŠ è½½ include è·¯å¾„å¤±è´¥: {e}")
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œé‡æ–°è®¡ç®—
        all_includes = set()
        
        # ä¼˜åŒ–ï¼šç›´æ¥ä» compile_db ä¸­æå–æ‰€æœ‰ -I è·¯å¾„
        # è¿™æ¯”é€ä¸ªæ–‡ä»¶æŸ¥æ‰¾å¿«å¾—å¤š
        include_pattern = r'-I\s*([^\s]+)'
        
        print(f"    [æå–] ä» compile_db æå–æ‰€æœ‰ include è·¯å¾„...")
        processed_entries = 0
        for entry in self.compile_db:
            processed_entries += 1
            if processed_entries > 0 and processed_entries % 10000 == 0:
                print(f"      å·²å¤„ç† {processed_entries}/{len(self.compile_db)} ä¸ªæ¡ç›®...")
            
            command = entry.get('command', '')
            if not command:
                continue
            
            # æå–æ‰€æœ‰ -I è·¯å¾„
            include_paths = re.findall(include_pattern, command)
            base_dir = self._resolve_entry_directory(entry)
            
            for inc_path_str in include_paths:
                inc_path = self._normalize_path(inc_path_str, base_dir)
                if inc_path.exists():
                    all_includes.add(inc_path)
        
        print(f"    âœ“ ä» {len(self.compile_db)} ä¸ªæ¡ç›®ä¸­æå–äº† {len(all_includes)} ä¸ªå”¯ä¸€ include è·¯å¾„")
        
        # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
        self._all_include_dirs_cache = all_includes
        
        # ä¿å­˜åˆ°æ–‡ä»¶ç¼“å­˜ï¼ˆæ›´æ–°ç°æœ‰ç¼“å­˜æ–‡ä»¶æˆ–åˆ›å»ºæ–°ç¼“å­˜ï¼‰
        try:
            cache_data = self._load_cache(cache_file)
            if not cache_data:
                # å¦‚æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„ç¼“å­˜æ•°æ®
                cache_data = {
                    'compile_db': self.compile_db,
                    'file_index': self.file_index,
                    'compile_db_path': str(self.compile_db_path.resolve())
                }
            # æ·»åŠ  include è·¯å¾„
            cache_data['all_include_dirs'] = [str(p) for p in all_includes]
            self._save_cache(cache_file, cache_data)
        except Exception as e:
            logger.debug(f"ä¿å­˜ include è·¯å¾„åˆ°ç¼“å­˜å¤±è´¥: {e}")
        
        return all_includes

    def find_first_source_file_containing(self, subpath: str) -> Optional[Path]:
        """
        åœ¨ç¼–è¯‘æ•°æ®åº“ä¸­æŸ¥æ‰¾ç¬¬ä¸€ä¸ªè·¯å¾„åŒ…å« subpath çš„æºæ–‡ä»¶ã€‚

        å…¸å‹ç”¨é€”ï¼šæŸäº› SelfContained æ¨¡å—åœ¨å½“å‰äº§å“é…ç½®ä¸‹å¹¶ä¸å‚ä¸ç¼–è¯‘ï¼Œ
        å› è€Œ compile_commands.json é‡Œæ²¡æœ‰å®ƒä»¬è‡ªå·±çš„ .c æ¡ç›®ã€‚
        è¿™æ—¶ä»å¯é€‰å–åŒä¸€å­ç³»ç»Ÿï¼ˆä¾‹å¦‚ kernel/liteos_aï¼‰çš„ä»»æ„ä¸€ä¸ª TU ä½œä¸ºâ€œç¼–è¯‘ä¸Šä¸‹æ–‡ä»£ç†â€ï¼Œ
        ä»¥è·å¾—æ›´æ¥è¿‘çœŸå®æ„å»ºçš„ include é¡ºåºä¸å®å®šä¹‰ã€‚

        Args:
            subpath: å½¢å¦‚ "kernel/liteos_a" çš„å­è·¯å¾„ç‰‡æ®µï¼ˆä½¿ç”¨ '/' åˆ†éš”ï¼‰

        Returns:
            åŒ¹é…åˆ°çš„æºæ–‡ä»¶è·¯å¾„ï¼ˆPathï¼‰æˆ– None
        """
        if not self.compile_db:
            return None

        needle = (subpath or "").replace("\\", "/")
        if not needle:
            return None

        # Memoize because this can be called repeatedly during bindgen retries / proxy TU selection.
        try:
            cache = getattr(self, "_first_source_file_containing_cache", None)
            if cache is None:
                cache = {}
                setattr(self, "_first_source_file_containing_cache", cache)
            if needle in cache:
                return cache[needle]
        except Exception:
            cache = None

        best: Optional[Path] = None
        best_exists = False

        for entry in self.compile_db:
            entry_path = self._resolve_entry_file_path(entry)
            if not entry_path:
                continue

            file_norm = str(entry_path).replace("\\", "/")
            if needle not in file_norm:
                continue

            # Only return C/C++ source files.
            if entry_path.suffix.lower() not in {".c", ".cc", ".cpp", ".cxx"}:
                continue

            # Prefer existing files to avoid misleading proxy paths and to support downstream readers.
            exists = False
            try:
                exists = entry_path.exists()
            except Exception:
                exists = False

            if best is None:
                best = entry_path
                best_exists = exists
                if best_exists:
                    # Fast path: the first existing match is usually good enough.
                    break
                continue

            if exists and not best_exists:
                best = entry_path
                best_exists = True
                break

        if cache is not None:
            try:
                cache[needle] = best
            except Exception:
                pass

        return best
    
    def has_file(self, source_file: Path) -> bool:
        """æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦åœ¨ç¼–è¯‘æ•°æ®åº“ä¸­"""
        source_file = Path(source_file).resolve()
        source_file_str = str(source_file)
        source_file_name = source_file.name
        
        # ç²¾ç¡®åŒ¹é…
        for entry in self.compile_db:
            if entry.get('file') == source_file_str:
                return True
        
        # æ–‡ä»¶ååŒ¹é…
        if source_file_name in self.file_index:
            return True
        
        return False
    
    # =========================================================================
    # å…¨å±€å¤´æ–‡ä»¶ç´¢å¼•ä¸æ™ºèƒ½å¯»è·¯ (Global Header Indexing & Smart Discovery)
    # åŸºäº Rustine è®ºæ–‡çš„ Build-Database Guided Discovery æ–¹æ³•
    # =========================================================================
    
    def find_header_path(
        self,
        header_name: str,
        *,
        preferred_subpaths: Optional[List[str]] = None,
        preferred_include_dirs: Optional[Iterable[Path]] = None,
    ) -> Optional[str]:
        """
        åœ¨æ‰€æœ‰å·²çŸ¥çš„ include ç›®å½•ä¸­æŸ¥æ‰¾ç‰¹å®šçš„å¤´æ–‡ä»¶
        
        è¿™æ˜¯è§£å†³ bindgen "file not found" é”™è¯¯çš„æ ¸å¿ƒæ–¹æ³•ã€‚
        å®ƒä¸æ˜¯é€ å‡ï¼ˆMockï¼‰ï¼Œè€Œæ˜¯åˆ©ç”¨æ„å»ºæ•°æ®åº“æ‰¾åˆ°å¤´æ–‡ä»¶çš„çœŸå®è·¯å¾„ã€‚
        
        Args:
            header_name: å¤´æ–‡ä»¶åï¼ˆå¯èƒ½åŒ…å«è·¯å¾„ï¼Œå¦‚ "core/hdf_device_desc.h"ï¼‰
        
        Returns:
            å¤´æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•è·¯å¾„ (str) æˆ– None
        """
        if not self._all_include_dirs_cache:
            self.get_all_include_dirs()

        # Load per-compile-db header resolution cache (small JSON).
        self._load_header_resolution_cache()
        cached_entry = self._header_resolution_cache.get(header_name) if isinstance(self._header_resolution_cache, dict) else None
        if isinstance(cached_entry, dict):
            cached_success = cached_entry.get("success")
            if isinstance(cached_success, str) and cached_success.strip():
                try:
                    d = Path(cached_success).expanduser()
                    if (d / header_name).exists():
                        return str(d)
                except Exception:
                    pass

        # Normalize hints
        norm_subpaths: List[Path] = []
        for s in (preferred_subpaths or []):
            s = (s or "").replace("\\", "/").strip().strip("/")
            if not s:
                continue
            norm_subpaths.append(Path(s))

        preferred_dirs_set: Set[Path] = set()
        try:
            if preferred_include_dirs:
                preferred_dirs_set = {Path(p) for p in preferred_include_dirs if p}
        except Exception:
            preferred_dirs_set = set()

        ohos_root_resolved: Optional[Path] = None
        if self.ohos_root:
            try:
                ohos_root_resolved = self.ohos_root.resolve()
            except Exception:
                ohos_root_resolved = self.ohos_root

        def _prefix_match_len(a: List[str], b: List[str]) -> int:
            n = 0
            for x, y in zip(a, b):
                if x != y:
                    break
                n += 1
            return n

        def _score_include_dir(p: Path) -> int:
            score = 0
            p_str = str(p).replace("\\", "/")
            p_low = p_str.lower()

            # Strong preference: already-known include dirs (from previous attempts).
            if preferred_dirs_set:
                try:
                    if p in preferred_dirs_set:
                        score += 10_000
                except Exception:
                    pass

            # Prefer dirs that lie under (or close to) the project's original subtree in OHOS tree.
            if norm_subpaths and ohos_root_resolved:
                try:
                    rel = Path(p).resolve().relative_to(ohos_root_resolved)
                    rel_parts = [x.lower() for x in rel.parts]
                except Exception:
                    rel_parts = []
                for sub in norm_subpaths:
                    hint_parts = [x.lower() for x in sub.parts]
                    if not hint_parts:
                        continue
                    # Prefix match is stronger than plain substring.
                    score += _prefix_match_len(rel_parts, hint_parts) * 200
                    # Substring match is a cheap fallback when rel_to fails.
                    hint_norm = str(sub).replace("\\", "/").lower()
                    if hint_norm and hint_norm in p_low:
                        score += 500

            return score

        def _dedupe_keep_order(xs: Iterable[str]) -> List[str]:
            out: List[str] = []
            seen: Set[str] = set()
            for x in xs:
                if not isinstance(x, str):
                    continue
                s = x.strip()
                if not s or s in seen:
                    continue
                seen.add(s)
                out.append(s)
            return out

        def _cache_add_candidates(header: str, include_dirs: List[Path]) -> None:
            if not header or not include_dirs:
                return
            try:
                entry = self._header_resolution_cache.get(header)
                if not isinstance(entry, dict):
                    entry = {}
                    self._header_resolution_cache[header] = entry
                existing = entry.get("candidates")
                if not isinstance(existing, list):
                    existing = []
                merged = _dedupe_keep_order([*existing, *[str(p) for p in include_dirs if p]])
                # Keep cache bounded to avoid huge JSON for very common headers (e.g. string.h).
                try:
                    max_keep = int(os.environ.get("C2R_HEADER_RESOLUTION_CACHE_MAX_CANDIDATES", "200"))
                except Exception:
                    max_keep = 200
                if max_keep > 0 and len(merged) > max_keep:
                    # Prefer higher-scored dirs under the current hint context.
                    try:
                        merged = sorted(merged, key=lambda s: _score_include_dir(Path(s)), reverse=True)[:max_keep]
                    except Exception:
                        merged = merged[:max_keep]
                entry["candidates"] = merged
                self._save_header_resolution_cache()
            except Exception as e:
                logger.debug(f"æ›´æ–° header resolution cache å¤±è´¥: {e}")

        # 1. ä¼˜å…ˆåœ¨ç´¢å¼•/ç¼“å­˜ä¸­æŸ¥æ‰¾ï¼ˆfast pathï¼›é¿å…é‡å¤æ‰«æå¤§æ ‘ï¼‰
        cand_dirs: List[str] = []
        if self._header_index and header_name in self._header_index:
            try:
                cand_dirs.extend([str(x) for x in (self._header_index.get(header_name) or [])])
            except Exception:
                pass
        if isinstance(cached_entry, dict):
            cands = cached_entry.get("candidates")
            if isinstance(cands, list):
                cand_dirs.extend([str(x) for x in cands if isinstance(x, str) and x.strip()])
        cand_dirs = _dedupe_keep_order(cand_dirs)
        if cand_dirs:
            best = None
            best_score = None
            for d in cand_dirs:
                try:
                    p = Path(d)
                except Exception:
                    continue
                sc = _score_include_dir(p)
                if best_score is None or sc > best_score:
                    best_score = sc
                    best = d
            if best:
                return best

        # 2. æ‡’æƒ°æ‰«æï¼šéå†æ‰€æœ‰ include ç›®å½•æŸ¥æ‰¾è¯¥æ–‡ä»¶
        # è¿™æ¯”æ„å»ºå…¨é‡ç´¢å¼•å¿«ï¼Œå› ä¸ºåªé’ˆå¯¹æŠ¥é”™çš„æ–‡ä»¶æ‰¾
        print(f"ğŸ” Hunting for missing header: {header_name} ...")

        matches: List[Path] = []
        best_dir: Optional[Path] = None
        best_score: Optional[int] = None

        for include_dir in self._all_include_dirs_cache:
            candidate = include_dir / header_name
            if not candidate.exists():
                continue

            matches.append(include_dir)
            sc = _score_include_dir(include_dir)
            if best_score is None or sc > best_score:
                best_score = sc
                best_dir = include_dir

        if best_dir is not None:
            print(f"ğŸ‰ Found {header_name} at {best_dir}")
            # ç¼“å­˜æ‰€æœ‰åŒ¹é…ï¼ˆåç»­åŒåç¼ºå¤±å¯ç›´æ¥æ‰“åˆ†é€‰æ‹©ï¼‰
            if self._header_index is None:
                self._header_index = defaultdict(list)
            for d in matches:
                self._header_index[header_name].append(str(d))
            _cache_add_candidates(header_name, matches)
            return str(best_dir)
        
        # 3. å¦‚æœç›´æ¥è·¯å¾„æ²¡æ‰¾åˆ°ï¼Œå°è¯•åªåŒ¹é…æ–‡ä»¶åï¼ˆå»æ‰å­ç›®å½•ï¼‰
        base_name = Path(header_name).name
        if base_name != header_name:
            print(f"ğŸ” Trying base name: {base_name} ...")
            for include_dir in self._all_include_dirs_cache:
                # åœ¨ include ç›®å½•çš„æ‰€æœ‰å­ç›®å½•ä¸­æŸ¥æ‰¾
                for candidate in include_dir.rglob(base_name):
                    if candidate.is_file():
                        # éªŒè¯å®Œæ•´è·¯å¾„æ˜¯å¦åŒ¹é…
                        parent_dir = candidate.parent
                        # è®¡ç®—éœ€è¦æ·»åŠ çš„ include è·¯å¾„
                        # ä¾‹å¦‚ï¼šæ‰¾åˆ° /a/b/c/core/hdf.hï¼Œheader_name="core/hdf.h"
                        # åˆ™éœ€è¦æ·»åŠ  /a/b/c
                        relative_parts = header_name.split('/')
                        if len(relative_parts) > 1:
                            # å‘ä¸Šå›æº¯æ‰¾åˆ°æ­£ç¡®çš„ include ç›®å½•
                            target_dir = parent_dir
                            for _ in range(len(relative_parts) - 1):
                                target_dir = target_dir.parent
                            # éªŒè¯
                            if (target_dir / header_name).exists():
                                print(f"ğŸ‰ Found {header_name} via rglob at {target_dir}")
                                if self._header_index is None:
                                    self._header_index = defaultdict(list)
                                self._header_index[header_name].append(str(target_dir))
                                _cache_add_candidates(header_name, [target_dir])
                                return str(target_dir)
        
        # 4. æœ€åå°è¯•ï¼šåœ¨ OpenHarmony æºç æ ‘ä¸­æœç´¢ï¼ˆBFS + æ·±åº¦å‰ªæï¼‰
        search_root = self._get_openharmony_search_root()
        if search_root and search_root.exists():
            try:
                max_depth = int(os.environ.get("C2R_MISSING_HEADER_SEARCH_MAX_DEPTH", "10"))
            except Exception:
                max_depth = 10
            max_depth = max(0, int(max_depth))
            print(f"ğŸ” Searching OpenHarmony source tree (depth<={max_depth}) for: {header_name} ...")

            try:
                relative_parts = header_name.split("/")
            except Exception:
                relative_parts = []

            matches: List[Path] = []
            best_dir: Optional[Path] = None
            best_score: Optional[int] = None

            q = deque([(Path(search_root), 0)])
            while q:
                cur_dir, depth = q.popleft()
                try:
                    with os.scandir(cur_dir) as it:
                        for ent in it:
                            try:
                                if ent.is_dir(follow_symlinks=False):
                                    if depth < max_depth:
                                        q.append((Path(ent.path), depth + 1))
                                    continue
                                if not ent.is_file(follow_symlinks=False):
                                    continue
                                if ent.name != base_name:
                                    continue

                                parent_dir = Path(ent.path).parent
                                if header_name == base_name:
                                    target_dir = parent_dir
                                    # No further verification needed: base_name matches.
                                else:
                                    if len(relative_parts) <= 1:
                                        continue
                                    target_dir = parent_dir
                                    for _ in range(len(relative_parts) - 1):
                                        target_dir = target_dir.parent
                                    if not (target_dir / header_name).exists():
                                        continue

                                matches.append(target_dir)
                                sc = _score_include_dir(target_dir)
                                if best_score is None or sc > best_score:
                                    best_score = sc
                                    best_dir = target_dir
                            except PermissionError:
                                continue
                            except Exception:
                                continue
                except PermissionError:
                    continue
                except FileNotFoundError:
                    continue
                except Exception as e:
                    logger.debug(f"Source tree BFS scan failed at {cur_dir}: {e}")
                    continue

            if best_dir is not None:
                print(f"ğŸ‰ Found {header_name} via source tree BFS at {best_dir}")
                if self._header_index is None:
                    self._header_index = defaultdict(list)
                for d in matches:
                    self._header_index[header_name].append(str(d))
                _cache_add_candidates(header_name, matches)
                return str(best_dir)
        
        print(f"âŒ Could not find header: {header_name}")
        return None
    
    def find_multiple_headers(self, header_names: List[str]) -> Dict[str, Optional[str]]:
        """
        æ‰¹é‡æŸ¥æ‰¾å¤šä¸ªå¤´æ–‡ä»¶
        
        Args:
            header_names: å¤´æ–‡ä»¶ååˆ—è¡¨
        
        Returns:
            å­—å…¸ { header_name: include_path_or_none }
        """
        results = {}
        for header in header_names:
            results[header] = self.find_header_path(header)
        return results
    
    def build_header_index(self, extensions: List[str] = None):
        """
        æ„å»ºå…¨å±€å¤´æ–‡ä»¶ç´¢å¼•ï¼ˆå®Œæ•´æ‰«æï¼‰
        
        è¿™æ˜¯ä¸€ä¸ªå¯é€‰çš„ä¼˜åŒ–ï¼šé¢„å…ˆæ‰«ææ‰€æœ‰ include ç›®å½•ï¼Œå»ºç«‹æ–‡ä»¶ååˆ°è·¯å¾„çš„æ˜ å°„ã€‚
        å¯¹äºå¤§å‹é¡¹ç›®ï¼ˆå¦‚ OpenHarmonyï¼‰ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œä½†åç»­æŸ¥æ‰¾ä¼šå¾ˆå¿«ã€‚
        
        Args:
            extensions: è¦ç´¢å¼•çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œé»˜è®¤ ['.h', '.hpp', '.hxx']
        """
        if self._header_index_built:
            return
        
        if extensions is None:
            extensions = ['.h', '.hpp', '.hxx', '.inc']
        
        if not self._all_include_dirs_cache:
            self.get_all_include_dirs()
        
        print(f"ğŸ”¨ Building global header index from {len(self._all_include_dirs_cache)} include directories...")
        
        self._header_index = defaultdict(list)
        total_headers = 0
        processed_dirs = 0
        
        for include_dir in self._all_include_dirs_cache:
            processed_dirs += 1
            if processed_dirs % 500 == 0:
                print(f"   Processed {processed_dirs}/{len(self._all_include_dirs_cache)} directories, found {total_headers} headers...")
            
            try:
                # åªæ‰«æç›´æ¥å­æ–‡ä»¶ï¼Œä¸é€’å½’ï¼ˆé¿å…é‡å¤ï¼‰
                for item in include_dir.iterdir():
                    if item.is_file() and item.suffix.lower() in extensions:
                        self._header_index[item.name].append(str(include_dir))
                        total_headers += 1
            except PermissionError:
                continue
            except Exception as e:
                logger.debug(f"Error scanning {include_dir}: {e}")
                continue
        
        self._header_index_built = True
        print(f"âœ… Header index built: {total_headers} headers in {len(self._header_index)} unique names")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        try:
            cache_file = self._get_cache_file()
            cache_data = self._load_cache(cache_file)
            if cache_data:
                cache_data['header_index'] = dict(self._header_index)
                self._save_cache(cache_file, cache_data)
        except Exception as e:
            logger.debug(f"Failed to save header index to cache: {e}")
    
    def get_resolved_includes_for_bindgen(
        self,
        missing_headers: List[str],
        current_includes: Set[Path],
        *,
        preferred_subpaths: Optional[List[str]] = None,
    ) -> Tuple[Set[Path], List[str], Dict[str, Optional[str]]]:
        """
        ä¸º bindgen è§£å†³ç¼ºå¤±çš„å¤´æ–‡ä»¶

        è¿™æ˜¯ä¾› skeleton_builder è°ƒç”¨çš„ä¾¿æ·æ–¹æ³•ã€‚

        Args:
            missing_headers: bindgen æŠ¥é”™ä¸­çš„ç¼ºå¤±å¤´æ–‡ä»¶åˆ—è¡¨
            current_includes: å½“å‰å·²æœ‰çš„ include è·¯å¾„é›†åˆ

        Returns:
            (æ–°çš„ include è·¯å¾„é›†åˆ, ä»ç„¶æ— æ³•è§£å†³çš„å¤´æ–‡ä»¶åˆ—è¡¨, è§£æç»“æœæ˜ å°„)
        """
        new_includes = set(current_includes)
        unresolved = []
        resolved_map: Dict[str, Optional[str]] = {}

        for header in missing_headers:
            found_dir = self.find_header_path(
                header,
                preferred_subpaths=preferred_subpaths,
                preferred_include_dirs=current_includes,
            )
            if found_dir:
                resolved_map[header] = found_dir
                found_path = Path(found_dir)
                if found_path not in new_includes:
                    print(f"âœ¨ Auto-resolved: {header} -> {found_dir}")
                    new_includes.add(found_path)
            else:
                resolved_map[header] = None
                unresolved.append(header)

        return new_includes, unresolved, resolved_map

    # =========================================================================
    # é¢„å¤„ç†ä¸Šä¸‹æ–‡é€‰æ‹©ä¸é¢„å¤„ç†åŠŸèƒ½ (Preprocess-First Approach)
    # =========================================================================

    def get_all_entries_for_file(self, source_file: Path) -> List[Dict]:
        """
        è·å–æºæ–‡ä»¶çš„æ‰€æœ‰ç¼–è¯‘æ•°æ®åº“æ¡ç›®

        åŒä¸€ä¸ªæºæ–‡ä»¶å¯èƒ½åœ¨ä¸åŒçš„ç¼–è¯‘é…ç½®ä¸‹æœ‰å¤šæ¡å‘½ä»¤ï¼ˆä¸åŒ board/modeï¼‰

        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„

        Returns:
            ç¼–è¯‘æ¡ç›®åˆ—è¡¨
        """
        if not self.compile_db:
            return []

        source_file = Path(source_file).resolve()
        source_file_str = str(source_file)
        source_file_name = source_file.name

        entries = []

        # æ–‡ä»¶ååŒ¹é…ï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰
        if source_file_name in self.file_index:
            candidates = self.file_index[source_file_name]
            for entry in candidates:
                entry_path = self._resolve_entry_file_path(entry)
                if entry_path and entry_path == source_file:
                    entries.append(entry)

            # å¦‚æœé€šè¿‡è§£ææ‰¾åˆ°ï¼Œè¿”å›
            if entries:
                return entries

            # å¦åˆ™è¿”å›æ‰€æœ‰å€™é€‰é¡¹ï¼ˆå¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼‰
            return candidates

        return entries

    def _find_proxy_entries_for_missing_file(
        self,
        source_file: Path,
        *,
        target_config: Optional[str] = None,
        max_depth: int = 6,
        max_files_per_dir: int = 40,
        max_entries: int = 30,
    ) -> List[Dict]:
        """
        When `source_file` is not present in compile_commands.json, try to find a "nearby" TU entry
        (same directory tree) and reuse its flags to preprocess `source_file`.

        This is a best-effort closure strategy: it does NOT claim semantic correctness, but it often
        restores macro/include visibility so downstream stages can operate on a stable `.i`.
        """
        try:
            source_file = Path(source_file).resolve()
        except Exception:
            source_file = Path(source_file)

        # Limit to common TU extensions.
        exts = {".c", ".cc", ".cpp", ".cxx", ".c++"}

        def _entry_key(e: Dict) -> str:
            try:
                return json.dumps(
                    {
                        "file": e.get("file"),
                        "directory": e.get("directory"),
                        "command": e.get("command"),
                        "arguments": e.get("arguments"),
                    },
                    sort_keys=True,
                )
            except Exception:
                return str(id(e))

        seen: Set[str] = set()
        proxy_entries: List[Dict] = []

        cur_dir = source_file.parent
        for _ in range(max(1, int(max_depth or 6))):
            try:
                if not cur_dir.exists() or not cur_dir.is_dir():
                    break
            except Exception:
                break

            # Scan a small number of source files in this directory as proxy candidates.
            try:
                files = [p for p in cur_dir.iterdir() if p.is_file() and p.suffix.lower() in exts]
                files.sort(key=lambda p: p.name)
            except Exception:
                files = []

            if files:
                files = files[: max(1, int(max_files_per_dir or 40))]

            for cand in files:
                if cand == source_file:
                    continue
                entries = self.get_all_entries_for_file(cand)
                if not entries:
                    continue
                # Prefer exact-path matched entries to avoid filename collision.
                for e in entries:
                    ep = self._resolve_entry_file_path(e)
                    if not ep or ep != cand.resolve():
                        continue
                    if target_config:
                        directory = e.get("directory", "") or ""
                        command = e.get("command", "") or ""
                        if target_config not in directory and target_config not in command:
                            continue
                    k = _entry_key(e)
                    if k in seen:
                        continue
                    seen.add(k)
                    proxy_entries.append(e)
                    if len(proxy_entries) >= max(1, int(max_entries or 30)):
                        break
                if len(proxy_entries) >= max(1, int(max_entries or 30)):
                    break

            if proxy_entries:
                break

            # Stop when reaching OHOS root (if configured).
            if self.ohos_root:
                try:
                    if cur_dir == self.ohos_root.resolve():
                        break
                except Exception:
                    pass
            # Walk up.
            cur_dir = cur_dir.parent

        return proxy_entries

    def preprocess_with_context(
        self,
        source_file: Path,
        entry: Dict,
        output_dir: Path = None,
        timeout_sec: int = 60,
    ) -> PreprocessingContext:
        """
        ä½¿ç”¨ç‰¹å®šç¼–è¯‘ä¸Šä¸‹æ–‡é¢„å¤„ç†æºæ–‡ä»¶

        è¿è¡Œ `clang -E` ç”Ÿæˆé¢„å¤„ç†åçš„ .i æ–‡ä»¶ï¼Œä¿ç•™ #line ä¿¡æ¯

        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„
            entry: compile_commands.json ä¸­çš„æ¡ç›®
            output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨ä¸´æ—¶ç›®å½•ï¼‰

        Returns:
            é¢„å¤„ç†ä¸Šä¸‹æ–‡å¯¹è±¡
        """
        source_file = Path(source_file).resolve()
        context = PreprocessingContext(source_file=source_file, entry=entry)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = Path(tempfile.gettempdir()) / "c2rust_preprocessed"
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼šæºæ–‡ä»¶å + entryå“ˆå¸Œ + .i
        entry_hash = hashlib.md5(json.dumps(entry, sort_keys=True).encode()).hexdigest()[:8]
        output_file = output_dir / f"{source_file.stem}_{entry_hash}.i"

        # è·å– clang å‚æ•°
        # å…³é”®ï¼šå¿…é¡»ä½¿ç”¨â€œæœ¬æ¬¡é€‰ä¸­çš„ entryâ€å¯¹åº”çš„å‚æ•°ï¼Œå¦åˆ™ active/best ç­–ç•¥ä¼šå¤±æ•ˆ
        clang_flags = self.get_clang_flags_for_entry(entry, normalize_paths=True)
        base_clang_flags = list(clang_flags or [])

        def _extract_missing_headers(stderr_text: str) -> List[str]:
            if not stderr_text:
                return []
            missing = re.findall(r"'([^']+)' file not found", stderr_text)
            missing += re.findall(r"fatal error:\s*([^\s:]+)\s*file not found", stderr_text)
            if not missing:
                return []
            seen = set()
            out: List[str] = []
            for f in missing:
                f = (f or "").strip().strip('"').strip("'").strip("<>").strip()
                if f and f not in seen:
                    seen.add(f)
                    out.append(f)
            return out

        def _include_dirs_from_flags(flags: List[str]) -> Set[Path]:
            dirs: Set[Path] = set()
            i = 0
            while i < len(flags):
                f = flags[i]
                if f == "-I" and i + 1 < len(flags):
                    p = (flags[i + 1] or "").strip()
                    if p:
                        dirs.add(Path(p))
                    i += 2
                    continue
                if f == "-isystem" and i + 1 < len(flags):
                    p = (flags[i + 1] or "").strip()
                    if p:
                        dirs.add(Path(p))
                    i += 2
                    continue
                if isinstance(f, str) and f.startswith("-I") and f != "-I":
                    p = f[2:].strip()
                    if p:
                        dirs.add(Path(p))
                if isinstance(f, str) and f.startswith("-isystem") and f != "-isystem":
                    p = f[len("-isystem") :].strip()
                    if p:
                        dirs.add(Path(p))
                i += 1
            return dirs

        def _append_include_flags(flags: List[str], include_dirs: Iterable[Path]) -> List[str]:
            out = list(flags or [])
            existing = _include_dirs_from_flags(out)
            for d in include_dirs:
                try:
                    p = Path(d)
                except Exception:
                    continue
                if p in existing:
                    continue
                out.extend(["-I", str(p)])
                existing.add(p)
            return out

        enable_auto = os.environ.get("C2R_PREPROCESS_AUTO_RESOLVE_INCLUDES", "1").strip().lower() in ("1", "true", "yes", "on")
        try:
            max_rounds = int(os.environ.get("C2R_PREPROCESS_AUTO_RESOLVE_ROUNDS", "100"))
        except Exception:
            max_rounds = 100
        max_rounds = max(0, max_rounds)

        preferred_subpaths: Optional[List[str]] = None
        if self.ohos_root:
            try:
                rel = source_file.resolve().relative_to(self.ohos_root.resolve())
                parts = list(rel.parts)
                max_parts = min(len(parts), 6)
                preferred_subpaths = [str(Path(*parts[:n])) for n in range(max_parts, 1, -1)]
            except Exception:
                preferred_subpaths = None

        include_dirs_base = _include_dirs_from_flags(base_clang_flags)
        # For headers with multiple candidates (e.g. los_config.h), we may need to try alternatives.
        resolved_include_by_header: Dict[str, str] = {}
        tried_include_by_header: Dict[str, Set[str]] = defaultdict(set)

        # æ„å»º clang -E å‘½ä»¤
        # ä¼˜å…ˆä½¿ç”¨ OpenHarmony é¢„ç½® clangï¼Œé¿å…å®¿ä¸»æœº clang å¯¹ OHOS target/sysroot çš„å…¼å®¹æ€§é—®é¢˜ã€‚
        clang_bin = shutil.which("clang") or "clang"
        if self.ohos_root:
            try:
                ohos_clang = (
                    Path(self.ohos_root)
                    / "prebuilts"
                    / "clang"
                    / "ohos"
                    / "linux-x86_64"
                    / "llvm"
                    / "bin"
                    / "clang"
                )
                if ohos_clang.exists():
                    clang_bin = str(ohos_clang)
            except Exception:
                pass

        last_err = ""
        last_missing: List[str] = []
        last_resolved_map: Dict[str, Optional[str]] = {}
        last_added: List[str] = []

        # NOTE: keep the output path stable across retries (same entry hash).
        total_rounds = max(1, max_rounds)
        for round_idx in range(1, total_rounds + 1):
            # Rebuild flags from the selected compile_commands entry each round, so multi-candidate
            # header fallback can REPLACE a previously-chosen include dir (instead of appending a wrong one).
            effective_flags = list(base_clang_flags)
            if resolved_include_by_header:
                effective_flags = _append_include_flags(
                    effective_flags,
                    (Path(p) for p in resolved_include_by_header.values() if p),
                )
            cmd = [clang_bin, "-E", str(source_file)]
            cmd.extend(effective_flags)
            # Keep diagnostics in logs, but avoid false failures when upstream uses `-Werror`.
            if "-Wno-error" not in effective_flags:
                cmd.append("-Wno-error")
            cmd.extend(["-o", str(output_file)])

            try:
                logger.debug(f"è¿è¡Œé¢„å¤„ç†å‘½ä»¤: {' '.join(cmd)}")
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=timeout_sec
                )

                if result.returncode != 0:
                    stderr = result.stderr or ""
                    last_err = stderr
                    if not enable_auto or round_idx >= total_rounds:
                        break
                    missing = _extract_missing_headers(stderr)
                    last_missing = list(missing)
                    if not missing:
                        break
                    # Resolve each missing header by trying cached candidates first, then discovering new ones.
                    resolved_map: Dict[str, Optional[str]] = {}
                    changed: List[str] = []

                    current_includes: Set[Path] = set(include_dirs_base)
                    for d in (resolved_include_by_header or {}).values():
                        try:
                            current_includes.add(Path(str(d)))
                        except Exception:
                            continue

                    for header in missing:
                        cand_dirs: List[str] = []
                        try:
                            self._load_header_resolution_cache()
                            cached = self._header_resolution_cache.get(header) if isinstance(self._header_resolution_cache, dict) else None
                            if isinstance(cached, dict):
                                suc = cached.get("success")
                                if isinstance(suc, str) and suc.strip():
                                    cand_dirs.append(suc.strip())
                                cands = cached.get("candidates")
                                if isinstance(cands, list):
                                    cand_dirs.extend([str(x) for x in cands if isinstance(x, str) and x.strip()])
                        except Exception:
                            cand_dirs = []

                        # If we don't have candidates yet, trigger a discovery pass (will populate cache candidates).
                        if not cand_dirs:
                            try:
                                _ = self.find_header_path(
                                    header,
                                    preferred_subpaths=preferred_subpaths,
                                    preferred_include_dirs=current_includes,
                                )
                            except Exception:
                                pass
                            try:
                                self._load_header_resolution_cache()
                                cached = self._header_resolution_cache.get(header) if isinstance(self._header_resolution_cache, dict) else None
                                if isinstance(cached, dict):
                                    cands = cached.get("candidates")
                                    if isinstance(cands, list):
                                        cand_dirs.extend([str(x) for x in cands if isinstance(x, str) and x.strip()])
                            except Exception:
                                pass

                        # De-dup in order
                        seen: Set[str] = set()
                        deduped: List[str] = []
                        for d in cand_dirs:
                            s = (d or "").strip()
                            if not s or s in seen:
                                continue
                            seen.add(s)
                            deduped.append(s)
                        cand_dirs = deduped

                        selected_dir: Optional[str] = None
                        for d in cand_dirs:
                            if d in tried_include_by_header.get(header, set()):
                                continue
                            try:
                                inc_dir = Path(d).expanduser()
                            except Exception:
                                continue
                            if not (inc_dir / header).exists():
                                continue
                            tried_include_by_header.setdefault(header, set()).add(d)
                            selected_dir = str(inc_dir)
                            break

                        if selected_dir:
                            prev = resolved_include_by_header.get(header)
                            resolved_include_by_header[header] = selected_dir
                            resolved_map[header] = selected_dir
                            if prev != selected_dir:
                                changed.append(selected_dir)
                        else:
                            resolved_map[header] = None

                    last_resolved_map = resolved_map or {}
                    last_added = list(changed)
                    if not changed:
                        break
                    logger.info(
                        f"clang -E ç¼ºå¤´æ–‡ä»¶ï¼Œå·²è‡ªåŠ¨è¡¥å…… include dirs: +{len(changed)} (round {round_idx}/{max_rounds})"
                    )
                    continue

                context.preprocessed_file = output_file

                # è§£æ #line æŒ‡ä»¤ï¼Œå»ºç«‹è¡Œå·æ˜ å°„
                context.line_mapping = self._parse_line_directives(output_file)

                # ç»Ÿè®¡å‡½æ•°æ•°é‡å’Œå®æ•°é‡
                context.function_count = self._count_functions_in_preprocessed(output_file)
                context.macro_count = len([flag for flag in base_clang_flags if str(flag).startswith("-D")])

                # If preprocessing succeeded with auto-resolved include dirs, cache the winning choice(s).
                if resolved_include_by_header:
                    try:
                        self._load_header_resolution_cache()
                        for header, inc in resolved_include_by_header.items():
                            if not header or not inc:
                                continue
                            ent = self._header_resolution_cache.get(header)
                            if not isinstance(ent, dict):
                                ent = {}
                                self._header_resolution_cache[header] = ent
                            ent["success"] = str(inc)
                            # Keep success at the front of candidates for future trials.
                            cands = ent.get("candidates")
                            if isinstance(cands, list):
                                merged = [str(inc), *[str(x) for x in cands if str(x) != str(inc)]]
                                ent["candidates"] = merged
                        self._save_header_resolution_cache()
                    except Exception:
                        pass

                logger.info(f"é¢„å¤„ç†æˆåŠŸ: {output_file.name} (å‡½æ•°: {context.function_count}, å®: {context.macro_count})")
                return context

            except subprocess.TimeoutExpired:
                context.error = f"é¢„å¤„ç†è¶…æ—¶ï¼ˆ{timeout_sec}ç§’ï¼‰"
                logger.warning(context.error)
                return context
            except Exception as e:
                context.error = f"é¢„å¤„ç†å¼‚å¸¸: {str(e)}"
                logger.warning(context.error)
                return context

        # Failed after retries
        err_lines = (last_err or "").splitlines()
        err_head = "\n".join(err_lines[:80]) if err_lines else (last_err or "")
        err_tail = "\n".join(err_lines[-40:]) if len(err_lines) > 120 else ""
        err = err_head
        if err_tail and err_tail not in err_head:
            err = err_head + "\n...\n" + err_tail
        err = (err or "").strip()
        if len(err) > 8000:
            err = err[:8000] + "\n...(truncated)..."
        context.error = f"é¢„å¤„ç†å¤±è´¥: {err}" if err else "é¢„å¤„ç†å¤±è´¥: unknown error"
        if enable_auto and (last_missing or last_added or last_resolved_map):
            context.auto_resolve = {
                "missing_headers": last_missing,
                "added_include_dirs": last_added,
                "resolved_map": last_resolved_map,
            }

        return context

    def _parse_line_directives(self, preprocessed_file: Path) -> Dict[int, Tuple[str, int]]:
        """
        è§£æé¢„å¤„ç†æ–‡ä»¶ä¸­çš„ #line æŒ‡ä»¤ï¼Œå»ºç«‹è¡Œå·æ˜ å°„

        #line æŒ‡ä»¤æ ¼å¼:
        # 123 "/path/to/original/file.c" flags

        Args:
            preprocessed_file: é¢„å¤„ç†åçš„ .i æ–‡ä»¶

        Returns:
            å­—å…¸: {.i æ–‡ä»¶è¡Œå·: (åŸæ–‡ä»¶è·¯å¾„, åŸæ–‡ä»¶è¡Œå·)}
        """
        line_mapping = {}

        try:
            with open(preprocessed_file, 'r', encoding='utf-8', errors='ignore') as f:
                current_original_file = None
                current_original_line = 0

                for i, line in enumerate(f, start=1):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ #line æŒ‡ä»¤
                    # æ ¼å¼: # 123 "/path/to/file.c" [flags]
                    match = re.match(r'^#\s+(\d+)\s+"([^"]+)"', line)
                    if match:
                        current_original_line = int(match.group(1))
                        current_original_file = match.group(2)
                        continue

                    # è®°å½•æ˜ å°„
                    if current_original_file:
                        line_mapping[i] = (current_original_file, current_original_line)
                        current_original_line += 1

        except Exception as e:
            logger.debug(f"è§£æ #line æŒ‡ä»¤å¤±è´¥: {e}")

        return line_mapping

    def _count_functions_in_preprocessed(self, preprocessed_file: Path) -> int:
        """
        ç»Ÿè®¡é¢„å¤„ç†æ–‡ä»¶ä¸­çš„å‡½æ•°å®šä¹‰æ•°é‡ï¼ˆç®€å•å¯å‘å¼ï¼‰

        ä½¿ç”¨ç®€å•çš„æ¨¡å¼åŒ¹é…ï¼Œé¿å…å¼•å…¥ tree-sitter ä¾èµ–

        Args:
            preprocessed_file: é¢„å¤„ç†åçš„ .i æ–‡ä»¶

        Returns:
            å‡½æ•°å®šä¹‰æ•°é‡ï¼ˆä¼°è®¡å€¼ï¼‰
        """
        try:
            with open(preprocessed_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # ç®€å•å¯å‘å¼ï¼šåŒ¹é…å‡½æ•°å®šä¹‰æ¨¡å¼
            # åŒ¹é…å½¢å¦‚: return_type function_name(...) {
            # æ’é™¤ typedefã€structã€unionã€enum
            pattern = r'\b(?!typedef|struct|union|enum|if|for|while|switch)\w+\s+\w+\s*\([^)]*\)\s*\{'
            matches = re.findall(pattern, content)

            return len(matches)

        except Exception as e:
            logger.debug(f"ç»Ÿè®¡å‡½æ•°æ•°é‡å¤±è´¥: {e}")
            return 0

    def select_preprocessing_context(
        self,
        source_file: Path,
        strategy: ContextSelectionStrategy = ContextSelectionStrategy.ACTIVE,
        target_config: str = None,
        output_dir: Path = None
    ) -> Optional[PreprocessingContext]:
        """
        æ ¹æ®ç­–ç•¥é€‰æ‹©æœ€ä½³é¢„å¤„ç†ä¸Šä¸‹æ–‡

        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„
            strategy: ä¸Šä¸‹æ–‡é€‰æ‹©ç­–ç•¥ï¼ˆé»˜è®¤ ACTIVEï¼‰
            target_config: ç›®æ ‡é…ç½®ï¼ˆç”¨äº ACTIVE ç­–ç•¥ï¼Œå¦‚ "rk3568"ï¼‰
            output_dir: é¢„å¤„ç†è¾“å‡ºç›®å½•

        Returns:
            é€‰æ‹©çš„é¢„å¤„ç†ä¸Šä¸‹æ–‡ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        def _pick_best(entries_to_try: List[Dict]) -> Tuple[Optional[PreprocessingContext], Optional[PreprocessingContext]]:
            best_context: Optional[PreprocessingContext] = None
            best_score = None
            last_failed: Optional[PreprocessingContext] = None
            for e in entries_to_try:
                ctx = self.preprocess_with_context(source_file, e, output_dir)
                if ctx.error:
                    last_failed = ctx
                    logger.debug(f"è·³è¿‡å¤±è´¥çš„é¢„å¤„ç†: {ctx.error}")
                    continue
                score = (ctx.function_count * 10 + ctx.macro_count, ctx.function_count, ctx.macro_count)
                if best_score is None or score > best_score:
                    best_score = score
                    best_context = ctx
            return best_context, last_failed

        # è·å–æ‰€æœ‰ç¼–è¯‘æ¡ç›®
        entries = self.get_all_entries_for_file(source_file)

        if not entries:
            # No compile_commands entry for this source file -> input closure is incomplete.
            # Default behavior: DO NOT try to "construct TU closure" via proxy-TU heuristics (brittle).
            # Users can explicitly re-enable proxy-TU fallback for legacy compatibility.
            logger.warning(f"æœªæ‰¾åˆ°æºæ–‡ä»¶çš„ç¼–è¯‘æ¡ç›®: {source_file}")
            proxy_env = os.environ.get("C2R_ENABLE_PROXY_TU_FALLBACK", "").strip().lower()
            if proxy_env:
                enable_proxy = proxy_env in ("1", "true", "yes", "on")
            else:
                truth_mode = os.environ.get("C2R_TRUTH_MODE", "0").strip().lower() in ("1", "true", "yes", "on")
                enable_proxy = not truth_mode
            if not enable_proxy:
                return PreprocessingContext(
                    source_file=Path(source_file),
                    entry={},
                    preprocessed_file=None,
                    function_count=0,
                    macro_count=0,
                    line_mapping={},
                    error="compile_commands_missing_entry (proxy_tu_disabled)",
                    proxy_used=False,
                    proxy_entry_file=None,
                    proxy_reason=None,
                )
            # Optional: Proxy-TU fallback (best-effort). This may recover macro/include visibility but is not guaranteed correct.
            proxy_target = target_config if strategy == ContextSelectionStrategy.ACTIVE else None
            try:
                max_depth = int(os.environ.get("C2R_PROXY_TU_MAX_DEPTH", "6"))
                max_files = int(os.environ.get("C2R_PROXY_TU_MAX_FILES_PER_DIR", "40"))
                max_entries = int(os.environ.get("C2R_PROXY_TU_MAX_ENTRIES", "30"))
            except Exception:
                max_depth, max_files, max_entries = 6, 40, 30
            proxy_entries = self._find_proxy_entries_for_missing_file(
                Path(source_file),
                target_config=proxy_target,
                max_depth=max_depth,
                max_files_per_dir=max_files,
                max_entries=max_entries,
            )
            if proxy_entries:
                logger.warning(
                    f"Proxy-TU: ä½¿ç”¨é™„è¿‘ TU æ¡ç›®({len(proxy_entries)}) é¢„å¤„ç†ç¼ºå¤±æ–‡ä»¶: {Path(source_file).name}"
                )
                best_ctx, last_failed = _pick_best(proxy_entries)
                if best_ctx:
                    try:
                        best_ctx.proxy_used = True
                        best_ctx.proxy_entry_file = str(best_ctx.entry.get("file") or "")
                        best_ctx.proxy_reason = "compile_commands_missing_entry"
                    except Exception:
                        pass
                    return best_ctx
                if last_failed:
                    try:
                        last_failed.proxy_used = True
                        last_failed.proxy_entry_file = str(last_failed.entry.get("file") or "")
                        last_failed.proxy_reason = "compile_commands_missing_entry"
                    except Exception:
                        pass
                    return last_failed
            return PreprocessingContext(
                source_file=Path(source_file),
                entry={},
                preprocessed_file=None,
                function_count=0,
                macro_count=0,
                line_mapping={},
                error="compile_commands_missing_entry (proxy_tu_failed)",
                proxy_used=False,
                proxy_entry_file=None,
                proxy_reason=None,
            )

        logger.info(f"æ‰¾åˆ° {len(entries)} ä¸ªç¼–è¯‘é…ç½®ï¼Œç­–ç•¥: {strategy.value}")

        # å¦‚æœå­˜åœ¨â€œç²¾ç¡®è·¯å¾„å‘½ä¸­â€çš„æ¡ç›®ï¼Œä¼˜å…ˆä½¿ç”¨è¿™äº›ï¼Œé¿å…æ–‡ä»¶åç¢°æ’å¯¼è‡´çš„è¯¯é€‰
        try:
            src_resolved = Path(source_file).resolve()
            exact_entries = []
            for e in entries:
                ep = self._resolve_entry_file_path(e)
                if ep and ep == src_resolved:
                    exact_entries.append(e)
            if exact_entries:
                entries = exact_entries
                logger.info(f"ç²¾ç¡®è·¯å¾„å‘½ä¸­ {len(entries)} ä¸ªæ¡ç›®ï¼ˆå·²è¿‡æ»¤æ–‡ä»¶åç¢°æ’å€™é€‰ï¼‰")
        except Exception:
            pass

        # é˜²æ­¢æç«¯æƒ…å†µä¸‹ï¼ˆæ–‡ä»¶åç¢°æ’ï¼‰å°è¯•è¿‡å¤š entry å¯¼è‡´è€—æ—¶çˆ†ç‚¸
        try:
            max_entries = int(os.environ.get("C2R_CONTEXT_MAX_ENTRIES", "25"))
            if max_entries > 0 and len(entries) > max_entries:
                logger.warning(f"æ¡ç›®æ•°é‡è¿‡å¤š({len(entries)})ï¼Œä»…å°è¯•å‰ {max_entries} ä¸ªï¼ˆå¯ç”¨ C2R_CONTEXT_MAX_ENTRIES è°ƒæ•´ï¼‰")
                entries = entries[:max_entries]
        except Exception:
            pass

        # ACTIVE ç­–ç•¥ï¼šç”¨æˆ·æŒ‡å®šé…ç½®ï¼ˆout_dir/board/modeï¼‰ï¼Œåœ¨åŒ¹é…å­é›†ä¸­åš best-of-matching
        if strategy == ContextSelectionStrategy.ACTIVE:
            if not target_config:
                logger.warning("ACTIVE ç­–ç•¥éœ€è¦æŒ‡å®š target_config")
                # å›é€€åˆ° BEST ç­–ç•¥
                strategy = ContextSelectionStrategy.BEST
            else:
                matched = []
                for entry in entries:
                    directory = entry.get('directory', '') or ''
                    command = entry.get('command', '') or ''
                    if target_config in directory or target_config in command:
                        matched.append(entry)

                if matched:
                    logger.info(f"ACTIVE åŒ¹é…åˆ° {len(matched)} ä¸ªæ¡ç›®: {target_config}ï¼Œå¼€å§‹è‡ªåŠ¨è£å†³")
                    best_ctx, last_failed = _pick_best(matched)
                    if best_ctx:
                        logger.info(
                            f"é€‰æ‹© ACTIVE é…ç½®: {target_config} "
                            f"(å‡½æ•°={best_ctx.function_count}, å®={best_ctx.macro_count})"
                        )
                        return best_ctx
                    if last_failed:
                        logger.warning(f"ACTIVE åŒ¹é…åˆ°çš„æ¡ç›®å…¨éƒ¨é¢„å¤„ç†å¤±è´¥ï¼Œå°†è¿”å›å¤±è´¥ä¸Šä¸‹æ–‡ä»¥ä¾¿è¯Šæ–­: {target_config}")
                        return last_failed

                    logger.warning(f"ACTIVE åŒ¹é…åˆ°çš„æ¡ç›®å…¨éƒ¨é¢„å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ° BEST ç­–ç•¥: {target_config}")
                    strategy = ContextSelectionStrategy.BEST
                else:
                    logger.warning(f"æœªæ‰¾åˆ°åŒ¹é… {target_config} çš„é…ç½®ï¼Œå›é€€åˆ° BEST ç­–ç•¥")
                    strategy = ContextSelectionStrategy.BEST

        # BEST ç­–ç•¥ï¼šé€‰æ‹©äº§ç”Ÿæœ€å¤šå‡½æ•°/å®çš„é…ç½®
        if strategy == ContextSelectionStrategy.BEST:
            best_context, last_failed = _pick_best(entries)
            if best_context:
                logger.info(f"é€‰æ‹© BEST é…ç½®: å‡½æ•°={best_context.function_count}, å®={best_context.macro_count}")
                return best_context

            if last_failed:
                logger.warning("æ‰€æœ‰é¢„å¤„ç†é…ç½®å‡å¤±è´¥ï¼Œè¿”å›å¤±è´¥ä¸Šä¸‹æ–‡ä»¥ä¾¿è¯Šæ–­")
                return last_failed
            logger.warning("æ‰€æœ‰é¢„å¤„ç†é…ç½®å‡å¤±è´¥")
            return PreprocessingContext(
                source_file=Path(source_file),
                entry={},
                preprocessed_file=None,
                function_count=0,
                macro_count=0,
                line_mapping={},
                error="preprocess_all_failed (no context available)",
                proxy_used=False,
                proxy_entry_file=None,
                proxy_reason=None,
            )

        # UNION ç­–ç•¥ï¼šæš‚ä¸å®ç°ï¼ˆéœ€è¦æ›´å¤æ‚çš„åˆå¹¶é€»è¾‘å’Œ #[cfg] ç®¡ç†ï¼‰
        if strategy == ContextSelectionStrategy.UNION:
            logger.warning("UNION ç­–ç•¥æš‚æœªå®ç°ï¼Œå›é€€åˆ° BEST ç­–ç•¥")
            return self.select_preprocessing_context(
                source_file,
                ContextSelectionStrategy.BEST,
                target_config,
                output_dir
            )

        return None


def get_includes_from_compile_commands(
    compile_db_path: Path,
    source_file: Path,
    ohos_root: Path = None
) -> List[Path]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä» compile_commands.json è·å–æºæ–‡ä»¶çš„å¤´æ–‡ä»¶æœç´¢è·¯å¾„
    
    Args:
        compile_db_path: compile_commands.json çš„è·¯å¾„
        source_file: è¦æŸ¥æ‰¾çš„æºæ–‡ä»¶è·¯å¾„
        ohos_root: OpenHarmony æºç æ ¹ç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        å¤´æ–‡ä»¶æœç´¢è·¯å¾„åˆ—è¡¨
    """
    parser = CompileCommandsParser(compile_db_path, ohos_root)
    return parser.get_includes_for_file(source_file)


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python3 compile_commands_parser.py <compile_commands.json> <source_file>")
        sys.exit(1)
    
    compile_db_path = Path(sys.argv[1])
    source_file = Path(sys.argv[2])
    
    parser = CompileCommandsParser(compile_db_path)
    includes = parser.get_includes_for_file(source_file)
    
    print(f"æ–‡ä»¶: {source_file}")
    print(f"æ‰¾åˆ° {len(includes)} ä¸ª include è·¯å¾„:")
    for inc in includes[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
        print(f"  - {inc}")
